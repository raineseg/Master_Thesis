{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i2vI-CVtWxz3"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "#reader tables function (with filtration)\n",
        "def tsv_reader_filter(path):\n",
        "    sample = pd.read_csv(path, sep=\"\\t\")\n",
        "    filename = os.path.basename(path) #get file name\n",
        "    samplename = os.path.splitext(filename)[0] #get sample name\n",
        "    print(samplename)\n",
        "    sample = sample[sample.Status != \"Missing\"] #remowe missings\n",
        "    sample = sample[sample.Status != \"Fragmented\"] #remowe Fragmented\n",
        "\n",
        "    try: #for samples with coverage value\n",
        "        sample[['Node', 'Cov']] = sample['Sequence'].str.split('_cov_', expand=True) #split sequence name\n",
        "        sample['Cov'] = pd.to_numeric(sample['Cov']) #convert Cov column to numeric\n",
        "        sample = sample.groupby('Busco id', group_keys=False).apply(lambda x: x.loc[x.Cov.idxmax()]) #drop duplicates of busco id, only that with the largest coverage left\n",
        "        sample = sample.drop(['Cov', 'Node'], inplace=False, axis=1) #delete columns\n",
        "\n",
        "    except ValueError:\n",
        "        sample = sample.groupby('Busco id', group_keys=False).apply(lambda x: x.loc[x.Score.idxmax()]) #drop duplicates of busco id, only that with the largest Score left\n",
        " \n",
        "    sample['Sample'] = samplename #to add sample name\n",
        "    return sample\n",
        "\n",
        "#reader tables function (without filtration)\n",
        "def tsv_reader(path):\n",
        "  sample = pd.read_csv(path, sep=\"\\t\")\n",
        "  sample['Sample'] = path[-11:-4]\n",
        "  sample['Sample'] = sample['Sample'].str.replace(r'/', '') #sample name fix\n",
        "  sample = sample[sample.Status != \"Missing\"] #remowe missings\n",
        "  #sample = sample[sample.Status != \"Fragmented\"] #remowe Fragmented\n",
        "  return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import with filtration\n",
        "#/content/drive/MyDrive/full_tables/selected/\n",
        "dir = input(\"Provide the path to the sample directory: \")\n",
        "allpaths = os.path.join(dir, \"*.txt\") #path to dir with files\n",
        "allfiles = glob.glob(allpaths) #get list of files\n",
        "concat_df = pd.concat((tsv_reader_filter(file) for file in allfiles)) #read files, filter, add samplenames column and concatenate\n",
        "concat_df\n"
      ],
      "metadata": {
        "id": "YvshnmWAOJBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zscl74BRw1ev"
      },
      "outputs": [],
      "source": [
        "#import without(!!!) filtration\n",
        "dir = input(\"Provide the path to the sample directory: \")\n",
        "allpaths = os.path.join(dir, \"*.txt\") #path to dir with files\n",
        "allfiles = glob.glob(allpaths) #get list of files\n",
        "concat_df = pd.concat((tsv_reader(file) for file in allfiles)) #read files, filter, add samplenames column and concatenate\n",
        "concat_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPEO6S4Pu-5h"
      },
      "outputs": [],
      "source": [
        "# procession of concat table\n",
        "concat_df.rename(columns={'Busco id':'Busco_id'}, inplace=True) #rename Busco id column\n",
        "filtered_buscoids = concat_df.groupby('Busco_id').filter(lambda x: len(x) == 61) #left only IDs that are not missed in 100 samples\n",
        "unique_buscoids = filtered_buscoids.Busco_id.unique().tolist() #get array of unique IDs\n",
        "\n",
        "with open(\"/content/unique_buscoids.list\", \"w\") as txt_file: #save unique IDs\n",
        "  txt_file.write(\"Busco_id\" + \"\\n\")\n",
        "  for line in unique_buscoids:\n",
        "    txt_file.write(\"\".join(line) + \"\\n\")\n",
        "\n",
        "filtered_buscoids.to_csv('/content/filtered_buscoids.tsv', sep='\\t', index=False) #save output\n",
        "\n",
        "\n",
        "oldgenes = pd.read_csv(\"/content/drive/MyDrive/full_tables/90genes.list\") #import 90genes list\n",
        "newgenes = pd.read_csv(\"/content/unique_buscoids.list\") #import filtered busco IDs list\n",
        "compare = pd.merge(oldgenes, newgenes)\n",
        "#concat_df_90genes = pd.concat((tsv_reader(file) for file in allfiles)) #read files, add samplenames column and concatenate\n",
        "#concat_df_90genes.rename(columns={'Busco id':'Busco_id'}, inplace=True) #rename Busco id column\n",
        "#filtered_buscoids_90genes = pd.merge(concat_df_90genes, compare) #get only Busco ID from \"compare file\"\n",
        "filtered_buscoids_90genes = pd.merge(concat_df, oldgenes) #get only Busco ID from 90genes list\n",
        "filtered_buscoids_90genes.to_csv('/content/filtered_buscoids_90genes.tsv', sep='\\t', index=False) #save output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKjlh78mb1kB"
      },
      "outputs": [],
      "source": [
        "#list of 90genes per sample\n",
        "concat_df_filtered = concat_df[concat_df['Busco_id'].isin(oldgenes['Busco_id'])]\n",
        "gene_counts = concat_df_filtered.groupby('Sample')['Busco_id'].nunique()\n",
        "gene_counts.to_csv('/content/genes90persample.tsv', sep='\\t',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rexBUyxNYgMF"
      },
      "outputs": [],
      "source": [
        "#save per-samples count of orthologs from 90genes list with duplications\n",
        "counttab_wdups = pd.crosstab(filtered_buscoids_90genes[\"Sample\"], filtered_buscoids_90genes[\"Status\"]).T\n",
        "counttab_wdups.to_csv('/content/counttab_wdups.tsv', sep='\\t',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_9g4IpnDgEk"
      },
      "outputs": [],
      "source": [
        "#save #per-samples count of orthologs from 90genes list without duplications\n",
        "filtered_buscoids_90genes_nodups = pd.merge(filtered_buscoids, compare)\n",
        "pd.crosstab(filtered_buscoids_90genes_nodups[\"Sample\"], filtered_buscoids_90genes_nodups[\"Status\"]).T\n",
        "counttab_nodups = pd.crosstab(filtered_buscoids_90genes_nodups[\"Sample\"], filtered_buscoids_90genes_nodups[\"Status\"]).T\n",
        "counttab_nodups.to_csv('/content/counttab_nodups.tsv', sep='\\t',)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vUYCt2u-YSo"
      },
      "outputs": [],
      "source": [
        "#print number of orthologs, common for all samples\n",
        "unique_buscoids_90genes_nodups = filtered_buscoids.Busco_id.unique().tolist()\n",
        "len(unique_buscoids_90genes_nodups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg19l7mOlU-z"
      },
      "outputs": [],
      "source": [
        "#print orthologs, common for all samples\n",
        "newgenes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfZZ0C4CHPM_"
      },
      "outputs": [],
      "source": [
        "#orthologs from 90genes list, found in all samples\n",
        "compare = pd.merge(oldgenes, newgenes)\n",
        "compare.to_csv('/content/compare.tsv', sep='\\t', index=False)\n",
        "compare\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J1hM3eWJ-BN0mo0Pl7UrS_plCqPFp3v_",
      "authorship_tag": "ABX9TyML348SxiwY950fwXW+6IKH"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}